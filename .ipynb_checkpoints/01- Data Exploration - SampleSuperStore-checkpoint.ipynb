{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Data Exploration: Sample SuperStore Dataset</h1>\n",
    "\n",
    "![grf-bg.jpg](images/grf-bg.jpg)\n",
    "\n",
    "This notebook is intended to give some quick and brief overview on how to explore a dataset, which in this case is the Sample SuperStore dataset. And to my best understanding, it's coming from a fictional online e-commerce company annual sales figures. So let's now dive-in and try to explore the dataset further with the various built-in functionalities of the Panda's library for Python has to offer, while utilizing the [Jupyter Notebook](https://jupyter.org/) as the main IDE of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "First and foremost, this data exploration would assume you to have one proper and working installation of [Python](https://www.python.org/) programming language. Secondly you have the [Jupyter Notebook](https://jupyter.org/) and the [Pandas](https://pandas.pydata.org/) library installed on your workstation, that you have access to, throughout the course of the exploration. \n",
    "\n",
    "While the installation part for each of the mentioned pieces of software mentioned go beyond the scope of this tutorial, I would suggest that you head over to the official sites, and there you may discover further steps on how to download, install and setup the required programming language and libraries according to your operating systems. Once that you have settled with the whole installation and configuration issues, you may come back again to this page and follow along the instructions.\n",
    "\n",
    "1. [Python Official Website](https://www.python.org/).\n",
    "2. [Panda's Official Website](https://pandas.pydata.org/).\n",
    "3. [Jupyter Official Website](https://jupyter.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library\n",
    "\n",
    "Secondly we need to load the library onto our Jupyter Notebook environment. This data exploration would assume you to have a proper and working installation of [Python](https://www.python.org/) programming language, secondly you have the [Jupyter Notebook](https://jupyter.org/) and the [Pandas](https://pandas.pydata.org/) library installed at the same time which you have access to, throughout the course of the exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = pd.read_csv('data/superstore.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice form the above code, it's implying that we need to put our `superstore.csv` dataset on a directory called `data`. So once that you download the dataset, create a folder named `data` and put your `superstore.csv` file there on that particular directory or folder.\n",
    "\n",
    "While the remaining code would imply these instructions :\n",
    "\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`pd`* = stands for Panda, it's the convention the community is using.\n",
    "- *`.read_csv`* = is a method within to read the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Panda's built-in functionality, only showing 20 columns and 10 rows for each dataset, everytime time you try to display them in the view. If you notice from the tabular data above, the dataset get truncated with triple dots sign **'...'** both for the rows and the columns. And since this dataset has 10800 rows with 21 columns, it'll only show the first 10 records for the row, with only 20 columns to the right instead of 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping The Row ID\n",
    "\n",
    "The \"Row ID\" column is not really that informative, I think it would be safe enough for us to simply just delete them. That way, it would give us much more clarity over our dataset. \n",
    "\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`.drop`* = the method being used to drop column.\n",
    "- *`inplace=True`* = we used them to keep the changes onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.drop(\"Row ID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the previously defined data variable, the 'df_orders'\n",
    "df_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see, from the above table, we don't have the 'ROW ID' in place no longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change The Index Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And each time you're using `pd.read_csv('somedata.csv')` that would yield the dataset's actual rows and columns, and we certainly have quite an extensive records of data, as being displayed from the previous function. \n",
    "\n",
    "As you may notice, the first column isn't the actual `'Row ID`' column, rather it's the default built-in feature Panda's bringing into the dataset. Let's try to change that into something much more useful. Now, let's get back to the previous Panda's function, and add another parameter, the `'index_col'` to be exact.\n",
    "\n",
    "And as we wish to redo again with a clean dataset, let's call them again one more time the dataset with the <br>\n",
    "`'df_orders = pd.read_csv('data/superstore.csv', index_col='Order ID')'` function.<br>\n",
    "\n",
    "And continue with the '`df_orders'` syntax again. So don't be surprise if you see the '`Row ID'` column will reappear in the dataset since that would illustrate our objective, but this time the index column values have changed from the value coming from the `'Order ID`' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to read again from the superstore.cs, and added the index_col='Order ID', parameter.\n",
    "df_orders = pd.read_csv('data/superstore.csv', index_col='Order ID') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the previously defined data variable, the 'df_orders'\n",
    "df_orders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that we tried to add the additional parameter, as you may notice, the first column have changed to 'Order ID' column, rather then the previous Panda's built-in index column, and the other thing was, the fine print below each table now have changed, from 21 columns, to only 20 columns instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the Row ID & Change The Index\n",
    "\n",
    "On to our another objective, what if we wish to combine both of the features, with dropping the `'Row ID`' and to change the `'Index`' columns at the same time, so that we could get even leaner dataset to work with. With that kind of objective, we might need to combine both of the syntax to achive our objective.\n",
    "\n",
    "`'df_orders.drop(\"Row ID\", axis=1, inplace=True, index_col='Order ID')`'\n",
    "\n",
    "- `'.drop()`' = this method to drop a column from the dataset.\n",
    "- `'axis=1`' = is the attribution value, on the dataset.\n",
    "- `'inplace=True`' = we used them to keep our changes onward.\n",
    "- `'index_col`' = make the define column, as our newly active index column instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = pd.read_csv('data/superstore.csv', index_col='Order ID')\n",
    "df_orders.drop(\"Row ID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the dataset again.\n",
    "df_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as you may see, from the below printed information, we only have 19 columns remaining left, coming from the initial 21 columns previously being shown. Now you may ask, \"But how come it's down to 19 columns, while we recall we only dropped 1 column?\". The answer to that is due to the `'index_col`' method, whereas we defined the `'Order ID`' to settle as the Index of the dataset. Pandas doesn't count that to a column, rather just another indexing attribution in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Number Rows & Columns \n",
    "Let's try to set the maximum column and row to display, since by default the pandas library would display **10 records** of rows in total for a single dataset. The first 5 would coming from the top records, and the remaining would be coming from the last 5 records as a whole. But that's a little too much information anyone could digest in a short glimpse, why don't we just minimize them down to 5 records instead. The same thing with the columns view, whereas Pandas would display you 20 columns, but since our current dataset only have 19 of them, then that should be fine.\n",
    "\n",
    "- *`pd.set_option('display.max_columns', 20)`* = setting the default column's view.\n",
    "- *`pd.set_option('display.max_rows', 5)`* = setting the default row's view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to read from the superstore.csv\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to give it a go with the new setting.\n",
    "df_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset First 5 Rows\n",
    "Here's another Pandas built-in method that may come handy. When you fell like taking a quick peek of the first 5 recrods from the top, the following code would deliver you those ouputs.\n",
    "<br>\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`.head()`* = is the method to display the first five records of data coming from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset Last 5 Rows\n",
    "Much like the above previous syntax, the similar can be apply to the bottom 5 records coming from your dataset. And you guess it right, the syntax would be `.tail()` and that would give you the last 5 records from the dataset.\n",
    "<br>\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`.tail()`* = is the method to display the last five records of data coming from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset Structure\n",
    "\n",
    "Now that you have one finer understanding on the previous aspect of importing library, loading the dataset and manipulate the views of the rows and the columns, let's now move on to the Dataset structure aspect. Whereas it's also an important area, before continuing the journey of exploring the dataset further. The dataset you get from the wild, might not always have the proper structure and data types you need. And before you could do further analysis and manipulation, let's make sure that both the structure and data types have been taken care of properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rows & Columns\n",
    "\n",
    "Following are both the built-in method to achive our next objective, as we go more deeper over the analysis part of the dataset. Let's try to understand further of what how many rows and columns are there, we know this information from the previous part, but lucky for us, Pandas also provide us with a method di display the information in hand.\n",
    "\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`.shape()`* = is the method to display the number of rows and column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from other ways to know how many Rows and Columns available from your dataset, Pandas also have a builtin method to dispay those information. So now we understand that the dataset has the following total of information records.\n",
    "<br />\n",
    "- `10800` coloumns\n",
    "- `19` coloumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Columns\n",
    "\n",
    "Imagine that you're working with a large dataset, and by large, not just merely on the amounts of rows that it'd produce. But also on the amount of columns spreaded from left to right. Good thing we're only working a 19 columns (from previously 21 columns in our dataset), now wouldn't it be nice to have a method to display all the columns available in our dataset? Well the good news is, Pandas shipped with a builtin method just to achive that.\n",
    "\n",
    "\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`.columns`* = is the method to display all the columns available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the columns (features) names.\n",
    "df_orders.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns Modification\n",
    "### Renaming Columns \n",
    "\n",
    "The following code would imply these instructions:\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`.columns`* = is the method to rename the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to rename the column.\n",
    "df_orders.columns = ['OrderDate', 'ShipDate', 'ShipMode', 'CustomerID', 'CustomerName', 'Segment' , 'Country', 'City', 'State', 'PostalCode', 'Region', 'ProductID', 'Category', 'SubCategory', 'ProductName' , 'Sales', 'Quantity', 'Discount', 'Profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's just my way of doing things on EDA aspect, is trying to eliminate any white spaces on between columns name. It gets better and you may take a lot benefit from performing this method, as we move on to something more complicated exploration, not to mention, we'll benefit on the clarity aspect, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns Data Type\n",
    "\n",
    "Many times before you wish to explore further your columns in a dataset with some operation, you may need to make sure it's the correct data type that you're working with. For example, you wouldn't be able to do a division operation over a Timestamp data format, or multiply a String with with another string for that matter.  \n",
    "\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`.columns`* = is the method to display all the columns available in the dataset.\n",
    "- *`.dtypes()`* = is the method to display the data types from the dataset available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's print the columns data types.\n",
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect Columns Data Type\n",
    "\n",
    "As we can see from the above snippets, we have noticed there are couple of columns data types that were set incorrectly. For instance, the `OrderDate` data type was set to `object` data type instead of `datetime`, or the `PostalCode` was set to `float` data type, though you wouldn't do any calculation on top of it. Somewhere down the line with that kind of flaws, will lead us to even bigger problem if we don't try to fix them now. Let's try to patch those data types with the following methods.\n",
    "\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`.astype`* = is the method to change the coloumns data type in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to change the datatypes of the following column in the dataset.\n",
    "df_orders['OrderDate'] = df_orders['OrderDate'].astype('datetime64[ns]')\n",
    "df_orders['ShipDate'] = df_orders['ShipDate'].astype('datetime64[ns]')\n",
    "df_orders['PostalCode'] = df_orders['PostalCode'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's try to recheck them again, to see if the codes have worked as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the columns data types.\n",
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Statistic Figures\n",
    "The following code would imply these instructions\n",
    "- *`df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.\n",
    "- *`.describe`* = is the method to pull out some statistics figures from the dataset.\n",
    "- Short note, the `.describe` method would only work for numerical coloumn, and not categorical.\n",
    "- While for the `(include='all')`, would work on both numerical & categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing statistical information on the dataset\n",
    "df_orders.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing more statistical information on the dataset\n",
    "df_orders.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic Figures\n",
    "\n",
    "The following code would imply these instructions\n",
    "\n",
    "*- `df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.<br>\n",
    "*- `.count`* = is the count value to a specific column.<br>\n",
    "*- `.mean`* = is the mean value to a specific column.<br>\n",
    "*- `.std`* = is the std value to a specific column.<br>\n",
    "*- `.min`* = is the min value to a specific column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders[\"Sales\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders[\"Sales\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders[\"Sales\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders[\"Sales\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Dataset\n",
    "\n",
    "Once that we've satisfied with out results, let's export them a new CSV dataset, so we could work with them on the next notebook.\n",
    "\n",
    "*- `df_orders`* = is the name of the variable, that will be using throughout the example of this tutorial.<br>\n",
    "*- `.to_csv`* = is the export method to a CSV dataset.<br>\n",
    "*- `index = False`* = we need to define this index value set to False, since we don't want the index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.to_csv('data/df_orders_exported.csv') \n",
    "# index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go ahead and check your current working directory. you may find your `df_orders_exported.csv` there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've come a long way of exploring our `superstore.csv` dataset, it's time to dive a little bit deeper of what, both Python and Pandas capable of delivering. Let's try to create a custom class in Python by leveraging our builtin Pandas method available in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a class named `display_all`, by which later we call on the next command.\n",
    "\n",
    "def display_all(df_orders):\n",
    "    with pd.option_context(\"display.max_rows\", 20, \"display.max_columns\", 20): \n",
    "        display(df_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What it does basically, it creates a class named `display_all` and called the `df_orders` variable that we've defined earlier at the top of this jupyter notebook tutorial. Next, we call the `pd.option_context` method that would provide us with the `display.max_rows` and the `dispplay.max_columns` attributions. And lastly we combine them all in the `display()` method by the end of the class.\n",
    "\n",
    "Now that we've combine them all together, let's put them into action, and see what it delivers back to us this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_orders.head(10).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've defined the `display_all` from the previous class, we can now use it to explore further and combine them with different methods available in Pandas, much like the `.describe` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_orders.describe(include='all').T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "518px",
    "width": "695px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
